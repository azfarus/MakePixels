{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as  plt\n",
    "from IPython.display import clear_output\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_image_generation_metrics import get_fid , get_inception_score_and_fid_from_directory,get_inception_score_from_directory\n",
    "from pytorch_image_generation_metrics.fid_ref import calc_fid_ref\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def pil_to_tensor(img, size):\n",
    "    img = img.resize((size, size), Image.BICUBIC)\n",
    "    tensor = torch.tensor(torch.ByteTensor(bytearray(img.tobytes()))\n",
    "                            ).view(img.size[1], img.size[0], 3)\n",
    "    tensor = tensor.permute(2,0,1).float() / 255.0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pixelify(img_tensor, pixel_size=8):\n",
    "    \"\"\"\n",
    "    img_tensor: torch.Tensor of shape (1,3,H,W)\n",
    "    pixel_size: size of the \"pixels\" (larger = more blocky)\n",
    "    \"\"\"\n",
    "    _, c, h, w = img_tensor.shape\n",
    "\n",
    "    # downscale\n",
    "    small = F.interpolate(img_tensor, size=(h//pixel_size, w//pixel_size), mode='nearest')\n",
    "    \n",
    "    # upscale back to original resolution\n",
    "    pixelated = F.interpolate(small, size=(h, w), mode='nearest')\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "\n",
    "def show_two_images(tensor1, tensor2, nrow=1, titles=(\"Image1\", \"Image2\")):\n",
    "    \"\"\"\n",
    "    Show two batches of images side by side.\n",
    "    tensor1, tensor2: [B,C,H,W] PyTorch tensors\n",
    "    \"\"\"\n",
    "    def denormalize(tensor):\n",
    "        return (tensor * 0.5) + 0.5  # from [-1,1] to [0,1]\n",
    "\n",
    "    # make grids from each tensor\n",
    "    grid1 = vutils.make_grid(denormalize(tensor1).cpu(), nrow=nrow)\n",
    "    grid2 = vutils.make_grid(denormalize(tensor2).cpu(), nrow=nrow)\n",
    "\n",
    "    npimg1 = grid1.permute(1, 2, 0).detach().numpy()\n",
    "    npimg2 = grid2.permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "    # plot side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(npimg1)\n",
    "    plt.axis(\"off\")\n",
    "    if titles and len(titles) > 0:\n",
    "        plt.title(titles[0])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(npimg2)\n",
    "    plt.axis(\"off\")\n",
    "    if titles and len(titles) > 1:\n",
    "        plt.title(titles[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic building blocks ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# --- Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=9):\n",
    "        super().__init__()\n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, out_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelShuffle Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- PixelShuffle Residual Block ---\n",
    "class PSResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# --- PixelShuffle Upsample Block ---\n",
    "class PixelShuffleUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, upscale_factor=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels * (upscale_factor ** 2),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.ps = nn.PixelShuffle(upscale_factor)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ps(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# --- Generator with PixelShuffle + PSResBlock ---\n",
    "class PixelShuffleResGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=9, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, base_channels, kernel_size=7),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = base_channels\n",
    "        for _ in range(2):\n",
    "            out_features = in_features * 2\n",
    "            layers += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # PSResBlocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            layers += [PSResBlock(in_features)]\n",
    "\n",
    "        # Upsampling with PixelShuffle\n",
    "        for _ in range(2):\n",
    "            out_features = in_features // 2\n",
    "            layers += [PixelShuffleUpsample(in_features, out_features, upscale_factor=2)]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(base_channels, out_channels, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            if use_dropout:\n",
    "                self.block.add_module(\"dropout\", nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_filters=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = UNetBlock(in_channels, base_filters, down=True)\n",
    "        self.down2 = UNetBlock(base_filters, base_filters*2, down=True)\n",
    "        self.down3 = UNetBlock(base_filters*2, base_filters*4, down=True)\n",
    "        self.down4 = UNetBlock(base_filters*4, base_filters*8, down=True)\n",
    "        self.down5 = UNetBlock(base_filters*8, base_filters*8, down=True)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_filters*8, base_filters*8, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = UNetBlock(base_filters*8, base_filters*8, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(base_filters*16, base_filters*8, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(base_filters*16, base_filters*4, down=False)\n",
    "        self.up4 = UNetBlock(base_filters*8, base_filters*2, down=False)\n",
    "        self.up5 = UNetBlock(base_filters*4, base_filters, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_filters*2, out_channels, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "\n",
    "        # upsample with skip connections\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d5], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d4], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d3], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d2], dim=1))\n",
    "        return self.final(torch.cat([u5, d1], dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelArt UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PixelArtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),  # no strided conv\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            # Nearest-neighbor upsampling + conv\n",
    "            self.block = nn.Sequential(\n",
    "                # nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class PixelArtUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (no stride, just convs)\n",
    "        self.enc1 = PixelArtBlock(in_channels, base_channels, down=True)\n",
    "        self.enc2 = PixelArtBlock(base_channels, base_channels*2, down=True)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = PixelArtBlock(base_channels*2, base_channels, down=False)\n",
    "\n",
    "        # Final conv\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(base_channels + base_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        b = self.bottleneck(e2)\n",
    "        d1 = self.dec1(b)\n",
    "        # skip connection\n",
    "        out = self.final(torch.cat([d1, e1], dim=1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Capacity Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
    "        self.style_scale = nn.Linear(w_dim, channels)\n",
    "        self.style_bias = nn.Linear(w_dim, channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        normalized = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * normalized + style_bias\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    \"\"\"Adds channel-wise noise with learnable scaling\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "        return x + self.weight * noise\n",
    "\n",
    "class GeneratorBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator block with convolution, AdaIN, and noise injection\n",
    "    (no upsampling so resolution stays the same)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.adain = AdaIN(out_channels, w_dim)\n",
    "        self.noise = NoiseInjection(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.conv(x)\n",
    "        x = self.adain(x, w)\n",
    "        x = self.noise(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class HighCapGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    High-capacity generator that takes an RGB image and outputs\n",
    "    an RGB image of the same resolution, using a learnable style vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=3, w_dim=512,\n",
    "                 base_channels=512, resolution=256, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable style vector w\n",
    "        self.w = nn.Parameter(torch.randn(1, w_dim))\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.to_rgb = nn.Conv2d(base_channels, 3, kernel_size=1)\n",
    "\n",
    "        in_ch = input_channels\n",
    "        out_ch = base_channels\n",
    "        for _ in range(num_blocks):\n",
    "            self.blocks.append(GeneratorBlock(in_ch, out_ch, w_dim))\n",
    "            in_ch = out_ch  # subsequent blocks use base_channels in/out\n",
    "\n",
    "        self.final_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_img):\n",
    "        \"\"\"\n",
    "        x_img: [B, 3, H, W] input image\n",
    "        \"\"\"\n",
    "        # broadcast w to batch size\n",
    "        w = self.w.expand(x_img.size(0), -1)  # [B, w_dim]\n",
    "\n",
    "        x = x_img\n",
    "        for block in self.blocks:\n",
    "            x = block(x, w)\n",
    "        x = self.to_rgb(x)\n",
    "        return self.final_activation(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# ---- user parameters ----\n",
    "resolution = 256                        # your desired resolution\n",
    "input_folder = \"landscape_dataset/testA\"                   # folder of input images\n",
    "real_folder = \"landscape_dataset/trainB\"            # folder of real images for comparison\n",
    "model_name = \"pixelart_unet\"                    # one of your model names\n",
    "epoch_num = 60                                  # for path\n",
    "# -------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelart = str.join('/',os.getcwd().split(\"\\\\\")) +'/'+ real_folder\n",
    "fid_file = pixelart + f\"/real_trainB_stats_{resolution}.npz\"\n",
    "\n",
    "# calc_fid_ref(pixelart, fid_file,img_size=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samaz\\AppData\\Local\\Temp\\ipykernel_8360\\2940997624.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path, map_location=device)\n",
      "C:\\Users\\samaz\\AppData\\Local\\Temp\\ipykernel_8360\\2201560245.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(torch.ByteTensor(bytearray(img.tobytes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception mean: 3.9572978314313616, Inception Var: 0.25296477131024947, FID: 145.25289658036303, model: highcap\n"
     ]
    }
   ],
   "source": [
    "# \"resnet\" ,\"pixelart_unet\",\n",
    "# \"interpolation\" ,  \"shuffle_residual\", \"highcap\" ,\n",
    "for model_name in [   \"highcap\"   ]:\n",
    "\n",
    "    if model_name != \"interpolation\":\n",
    "        if model_name == \"highcap\":\n",
    "            \n",
    "            epoch_num = 27\n",
    "            model = HighCapGenerator().to(device)\n",
    "        elif model_name == \"normal_unet\":\n",
    "            \n",
    "            epoch_num = 100\n",
    "            model = UNetGenerator().to(device)\n",
    "        elif model_name == \"pixelart_unet\":\n",
    "            \n",
    "            epoch_num = 100\n",
    "            model = PixelArtUNet().to(device)\n",
    "        elif model_name == \"shuffle_residual\":\n",
    "            \n",
    "            epoch_num = 99\n",
    "            model = PixelShuffleResGenerator().to(device)\n",
    "        elif model_name == \"resnet\":\n",
    "            \n",
    "            epoch_num = 50\n",
    "            model = Generator().to(device)\n",
    "\n",
    "\n",
    "        model_path = f\"models/landscape_dataset_{model_name}/epoch_{epoch_num}/G_XtoY.pth\"\n",
    "        save_folder = f\"landscape_dataset/generated_images_{model_name}_{resolution}\"  \n",
    "        os.makedirs(save_folder, exist_ok=True) \n",
    "\n",
    "        state = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = identity\n",
    "        save_folder = f\"landscape_dataset/generated_images_{model_name}_{resolution}\"  \n",
    "        os.makedirs(save_folder, exist_ok=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            pil_img = Image.open(os.path.join(input_folder, fname)).convert(\"RGB\")\n",
    "            img_tensor = pil_to_tensor(pil_img, resolution).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = model(img_tensor)\n",
    "\n",
    "            fake = (fake.cpu()+1.0)/2.0\n",
    "            fake = pixelify(fake, pixel_size=3)  # change 8 to control block size\n",
    "\n",
    "            # save generated image\n",
    "            out_pil = Image.fromarray(\n",
    "                (fake.squeeze(0).permute(1,2,0).numpy() * 255).astype(\"uint8\")\n",
    "            )\n",
    "            out_pil.save(os.path.join(save_folder, f\"gen_{fname}\"))\n",
    "\n",
    "\n",
    "\n",
    "for model_name in [   \"highcap\"  ]:\n",
    "\n",
    "\n",
    "    save_folder = f\"landscape_dataset/generated_images_{model_name}_{resolution}\"  \n",
    "    os.makedirs(save_folder, exist_ok=True) \n",
    "\n",
    "\n",
    "    generated = str.join('/',os.getcwd().split(\"\\\\\")) +'/'+ save_folder\n",
    "\n",
    "\n",
    "    ((im,iv),fid) = get_inception_score_and_fid_from_directory(generated,fid_file)\n",
    "    print(f\"Inception mean: {im}, Inception Var: {iv}, FID: {fid}, model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixelart = str.join('/',os.getcwd().split(\"\\\\\")) +'/'+ real_folder\n",
    "# generated = str.join('/',os.getcwd().split(\"\\\\\")) +'/'+ save_folder\n",
    "\n",
    "# fid_file = pixelart + \"/real_trainB_stats.npz\"\n",
    "\n",
    "# ((im,iv),fid) = get_inception_score_and_fid_from_directory(generated,fid_file)\n",
    "# print(f\"Inception mean: {im}, Inception Var: {iv}, FID: {fid}, model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixelart = str.join('/',os.getcwd().split(\"\\\\\")) +'/'+ real_folder\n",
    "# fid_file = pixelart + \"/real_trainB_stats.npz\"\n",
    "\n",
    "# calc_fid_ref(pixelart, fid_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "margenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
