{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as  plt\n",
    "from IPython.display import clear_output\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "resolution = 256\n",
    "gen_type =\"shuffle\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_two_images(tensor1, tensor2, nrow=1, titles=(\"Image1\", \"Image2\")):\n",
    "    \"\"\"\n",
    "    Show two batches of images side by side.\n",
    "    tensor1, tensor2: [B,C,H,W] PyTorch tensors\n",
    "    \"\"\"\n",
    "    def denormalize(tensor):\n",
    "        return (tensor * 0.5) + 0.5  # from [-1,1] to [0,1]\n",
    "\n",
    "    # make grids from each tensor\n",
    "    grid1 = vutils.make_grid(denormalize(tensor1).cpu(), nrow=nrow)\n",
    "    grid2 = vutils.make_grid(denormalize(tensor2).cpu(), nrow=nrow)\n",
    "\n",
    "    npimg1 = grid1.permute(1, 2, 0).detach().numpy()\n",
    "    npimg2 = grid2.permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "    # plot side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(npimg1)\n",
    "    plt.axis(\"off\")\n",
    "    if titles and len(titles) > 0:\n",
    "        plt.title(titles[0])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(npimg2)\n",
    "    plt.axis(\"off\")\n",
    "    if titles and len(titles) > 1:\n",
    "        plt.title(titles[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic building blocks ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, 3),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# --- Generator ---\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=9):\n",
    "        super().__init__()\n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, out_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelShuffle Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- PixelShuffle Residual Block ---\n",
    "class PSResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# --- PixelShuffle Upsample Block ---\n",
    "class PixelShuffleUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, upscale_factor=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels * (upscale_factor ** 2),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.ps = nn.PixelShuffle(upscale_factor)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ps(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# --- Generator with PixelShuffle + PSResBlock ---\n",
    "class PixelShuffleResGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=9, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, base_channels, kernel_size=7),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = base_channels\n",
    "        for _ in range(2):\n",
    "            out_features = in_features * 2\n",
    "            layers += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # PSResBlocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            layers += [PSResBlock(in_features)]\n",
    "\n",
    "        # Upsampling with PixelShuffle\n",
    "        for _ in range(2):\n",
    "            out_features = in_features // 2\n",
    "            layers += [PixelShuffleUpsample(in_features, out_features, upscale_factor=2)]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(base_channels, out_channels, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            if use_dropout:\n",
    "                self.block.add_module(\"dropout\", nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_filters=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = UNetBlock(in_channels, base_filters, down=True)\n",
    "        self.down2 = UNetBlock(base_filters, base_filters*2, down=True)\n",
    "        self.down3 = UNetBlock(base_filters*2, base_filters*4, down=True)\n",
    "        self.down4 = UNetBlock(base_filters*4, base_filters*8, down=True)\n",
    "        self.down5 = UNetBlock(base_filters*8, base_filters*8, down=True)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_filters*8, base_filters*8, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = UNetBlock(base_filters*8, base_filters*8, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(base_filters*16, base_filters*8, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(base_filters*16, base_filters*4, down=False)\n",
    "        self.up4 = UNetBlock(base_filters*8, base_filters*2, down=False)\n",
    "        self.up5 = UNetBlock(base_filters*4, base_filters, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_filters*2, out_channels, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "\n",
    "        # upsample with skip connections\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d5], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d4], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d3], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d2], dim=1))\n",
    "        return self.final(torch.cat([u5, d1], dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Encoder ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 128, 4, stride=2, padding=1),  # 2x downsample\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),           # 4x downsample\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, hidden_dim, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # optional extra convs for capacity:\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Codebook / Vector Quantizer ---\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings=1024, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "\n",
    "    def forward(self, z):\n",
    "        B, C, H, W = z.shape\n",
    "        z_flat = z.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "        dist = (\n",
    "            z_flat.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * z_flat @ self.embedding.weight.t()\n",
    "            + self.embedding.weight.pow(2).sum(1)\n",
    "        )\n",
    "        indices = dist.argmin(1)\n",
    "        quantized = self.embedding(indices).view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
    "        return quantized, indices.view(B, H, W)\n",
    "\n",
    "# --- Decoder ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels=3, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(hidden_dim, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, out_channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_q):\n",
    "        return self.model(z_q)\n",
    "\n",
    "# --- Latent Translator (CycleGAN generator in latent space) ---\n",
    "class LatentTranslator(nn.Module):\n",
    "    def __init__(self, dim=512, num_blocks=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            layers += [\n",
    "                nn.Conv2d(dim, dim, 3, padding=1),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z_q):\n",
    "        return self.model(z_q)\n",
    "\n",
    "# --- Putting it together for one direction X→Y ---\n",
    "class VQCycleGenerator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoderX=Encoder(),\n",
    "                 quantizerX=VectorQuantizer(),\n",
    "                 decoderY=Decoder(),\n",
    "                 translatorXY=LatentTranslator()):\n",
    "        super().__init__()\n",
    "        self.encoderX = encoderX\n",
    "        self.quantizerX = quantizerX\n",
    "        self.translatorXY = translatorXY\n",
    "        self.decoderY = decoderY\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_x = self.encoderX(x)                       # encode image X\n",
    "        z_qx, indices_x = self.quantizerX(z_x)       # quantize to codebook\n",
    "        z_latent_y = self.translatorXY(z_qx)         # translate latent to Y\n",
    "        out_y = self.decoderY(z_latent_y)            # decode into image Y\n",
    "        return out_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelArt UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PixelArtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),  # no strided conv\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            # Nearest-neighbor upsampling + conv\n",
    "            self.block = nn.Sequential(\n",
    "                # nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class PixelArtUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (no stride, just convs)\n",
    "        self.enc1 = PixelArtBlock(in_channels, base_channels, down=True)\n",
    "        self.enc2 = PixelArtBlock(base_channels, base_channels*2, down=True)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = PixelArtBlock(base_channels*2, base_channels, down=False)\n",
    "\n",
    "        # Final conv\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(base_channels + base_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        b = self.bottleneck(e2)\n",
    "        d1 = self.dec1(b)\n",
    "        # skip connection\n",
    "        out = self.final(torch.cat([d1, e1], dim=1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Capacity Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    \"\"\"Mapping network to transform latent space z to intermediate space W\"\"\"\n",
    "    def __init__(self, latent_dim=512, hidden_dim=512, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(latent_dim, hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.mapping(z)\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
    "        self.style_scale = nn.Linear(w_dim, channels)\n",
    "        self.style_bias = nn.Linear(w_dim, channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        normalized = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * normalized + style_bias\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    \"\"\"Adds channel-wise noise with learnable scaling\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "        return x + self.weight * noise\n",
    "\n",
    "class GeneratorBlock(nn.Module):\n",
    "    \"\"\"Generator block with upsampling, convolution, AdaIN, and noise injection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.adain = AdaIN(out_channels, w_dim)\n",
    "        self.noise = NoiseInjection(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.adain(x, w)\n",
    "        x = self.noise(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class HighCapGenerator(nn.Module):\n",
    "    \"\"\"High-capacity generator with mapping network and progressive synthesis\"\"\"\n",
    "    def __init__(self, latent_dim=512, w_dim=512, base_channels=512, max_resolution=1024):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(latent_dim, w_dim)\n",
    "        self.base = nn.Parameter(torch.randn(1, base_channels, 4, 4))  # Learned constant input\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.to_rgb = nn.ModuleList()\n",
    "\n",
    "        # Resolution stages: 4x4 -> 1024x1024\n",
    "        resolutions = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "        channels = [base_channels] + [base_channels // (2 ** i) for i in range(1, len(resolutions))]\n",
    "        \n",
    "        for i, (res, in_ch, out_ch) in enumerate(zip(resolutions[1:], channels, channels[1:])):\n",
    "            self.blocks.append(GeneratorBlock(in_ch, out_ch, w_dim))\n",
    "            self.to_rgb.append(nn.Conv2d(out_ch, 3, kernel_size=1))\n",
    "\n",
    "        self.final_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, target_resolution=1024):\n",
    "        w = self.mapping(z)\n",
    "        x = self.base.repeat(z.size(0), 1, 1, 1)\n",
    "        \n",
    "        for block, to_rgb in zip(self.blocks, self.to_rgb):\n",
    "            x = block(x, w)\n",
    "            if x.size(2) == target_resolution:\n",
    "                break\n",
    "\n",
    "        x = to_rgb(x)  # Convert to RGB\n",
    "        return self.final_activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            nn.Conv2d(256, 512, 4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDiscriminatorBlock(nn.Module):\n",
    "    \"\"\"Residual block for discriminator with downsampling\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        x = self.main(x)\n",
    "        return x + residual\n",
    "\n",
    "class ResidualDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator with residual blocks and spectral normalization\"\"\"\n",
    "    def __init__(self, base_channels=64, max_resolution=1024):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(3, base_channels, kernel_size=3, padding=1)),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.blocks = nn.ModuleList()\n",
    "        resolutions = [max_resolution // (2 ** i) for i in range(int(np.log2(max_resolution)) - 1)]\n",
    "        channels = [base_channels * (2 ** i) for i in range(len(resolutions))]\n",
    "        \n",
    "        for in_ch, out_ch in zip(channels, channels[1:]):\n",
    "            self.blocks.append(ResidualDiscriminatorBlock(in_ch, out_ch))\n",
    "        \n",
    "        self.final_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.utils.spectral_norm(nn.Linear(channels[-1], 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_pool(x).squeeze()\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_X, root_Y, transform=None):\n",
    "        self.files_X = [os.path.join(root_X, f) for f in os.listdir(root_X) if f.endswith(('jpg','png','jpeg'))]\n",
    "        self.files_Y = [os.path.join(root_Y, f) for f in os.listdir(root_Y) if f.endswith(('jpg','png','jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of iterations = max of both sets\n",
    "        return max(len(self.files_X), len(self.files_Y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # domain X image\n",
    "        img_X_path = self.files_X[index % len(self.files_X)]\n",
    "        img_X = Image.open(img_X_path).convert('RGB')\n",
    "\n",
    "        # random domain Y image (unaligned)\n",
    "        img_Y_path = random.choice(self.files_Y)\n",
    "        img_Y = Image.open(img_Y_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img_X = self.transform(img_X)\n",
    "            img_Y = self.transform(img_Y)\n",
    "\n",
    "        return img_X, img_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((resolution,resolution)),         # resize all images\n",
    "    transforms.ToTensor(),                # [0,1]\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # [-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder='landscape_dataset'\n",
    "\n",
    "dataset = ImageDataset(root_X=f'{dataset_folder}/trainA', root_Y=f'{dataset_folder}/trainB', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9663676416 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m G_XtoY = HighCapGenerator().to(device=device)\n\u001b[32m      3\u001b[39m G_YtoX = HighCapGenerator().to(device=device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m Disc_X = \u001b[43mResidualDiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device=device)\n\u001b[32m      5\u001b[39m Disc_Y = ResidualDiscriminator().to(device=device)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Optimizers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mResidualDiscriminator.__init__\u001b[39m\u001b[34m(self, base_channels, max_resolution)\u001b[39m\n\u001b[32m     32\u001b[39m channels = [base_channels * (\u001b[32m2\u001b[39m ** i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(resolutions))]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m in_ch, out_ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(channels, channels[\u001b[32m1\u001b[39m:]):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28mself\u001b[39m.blocks.append(\u001b[43mResidualDiscriminatorBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_ch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.final_pool = nn.AdaptiveAvgPool2d(\u001b[32m1\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.fc = nn.utils.spectral_norm(nn.Linear(channels[-\u001b[32m1\u001b[39m], \u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mResidualDiscriminatorBlock.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.residual = nn.Sequential(\n\u001b[32m      6\u001b[39m     nn.Conv2d(in_channels, out_channels, kernel_size=\u001b[32m1\u001b[39m, stride=\u001b[32m2\u001b[39m),\n\u001b[32m      7\u001b[39m     nn.LeakyReLU(\u001b[32m0.2\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.main = nn.Sequential(\n\u001b[32m     10\u001b[39m     nn.Conv2d(in_channels, out_channels, kernel_size=\u001b[32m3\u001b[39m, padding=\u001b[32m1\u001b[39m),\n\u001b[32m     11\u001b[39m     nn.LeakyReLU(\u001b[32m0.2\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     13\u001b[39m     nn.LeakyReLU(\u001b[32m0.2\u001b[39m),\n\u001b[32m     14\u001b[39m     nn.AvgPool2d(\u001b[32m2\u001b[39m)\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Installations\\MiniConda\\envs\\margenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:521\u001b[39m, in \u001b[36mConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[32m    520\u001b[39m dilation_ = _pair(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Installations\\MiniConda\\envs\\margenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:166\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m    159\u001b[39m         torch.empty(\n\u001b[32m    160\u001b[39m             (in_channels, out_channels // groups, *kernel_size),\n\u001b[32m    161\u001b[39m             **factory_kwargs,\n\u001b[32m    162\u001b[39m         )\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9663676416 bytes."
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "G_XtoY = HighCapGenerator().to(device=device)\n",
    "G_YtoX = HighCapGenerator().to(device=device)\n",
    "Disc_X = ResidualDiscriminator().to(device=device)\n",
    "Disc_Y = ResidualDiscriminator().to(device=device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(\n",
    "    list(G_XtoY.parameters()) + list(G_YtoX.parameters()), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_X = optim.Adam(Disc_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = optim.Adam(Disc_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Store losses for visualization\n",
    "losses_G, losses_D_X, losses_D_Y = [], [], []\n",
    "losses_cycle, losses_identity = [], []\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot([], [], label=\"Generator Loss\")\n",
    "line2, = ax.plot([], [], label=\"Discriminator X Loss\")\n",
    "line3, = ax.plot([], [], label=\"Discriminator Y Loss\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "iteration=0\n",
    "\n",
    "# Training loop skeleton\n",
    "for epoch in range(epochs):\n",
    "    for real_X, real_Y in dataloader:  # load mini-batches\n",
    "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
    "\n",
    "        # ----------------------\n",
    "        #  Train Generators\n",
    "        # ----------------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_Y = G_XtoY(real_X)\n",
    "        loss_GAN_XtoY = criterion_GAN(Disc_Y(fake_Y), torch.ones_like(Disc_Y(fake_Y)))\n",
    "\n",
    "        fake_X = G_YtoX(real_Y)\n",
    "        loss_GAN_YtoX = criterion_GAN(Disc_X(fake_X), torch.ones_like(Disc_X(fake_X)))\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_X = G_YtoX(fake_Y)\n",
    "        recov_Y = G_XtoY(fake_X)\n",
    "        loss_cycle = criterion_cycle(recov_X, real_X) + criterion_cycle(recov_Y, real_Y)\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_X = criterion_identity(G_YtoX(real_X), real_X)\n",
    "        loss_id_Y = criterion_identity(G_XtoY(real_Y), real_Y)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN_XtoY + loss_GAN_YtoX + 10.0 * loss_cycle + 5.0 * (loss_id_X + loss_id_Y)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator X\n",
    "        # -----------------------\n",
    "        optimizer_D_X.zero_grad()\n",
    "\n",
    "        loss_real = criterion_GAN(Disc_X(real_X), torch.ones_like(Disc_X(real_X)))\n",
    "        loss_fake = criterion_GAN(Disc_X(fake_X.detach()), torch.zeros_like(Disc_X(fake_X)))\n",
    "        loss_D_X = (loss_real + loss_fake) * 0.5\n",
    "        loss_D_X.backward()\n",
    "        optimizer_D_X.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator Y\n",
    "        # -----------------------\n",
    "        optimizer_D_Y.zero_grad()\n",
    "\n",
    "        loss_real = criterion_GAN(Disc_Y(real_Y), torch.ones_like(Disc_Y(real_Y)))\n",
    "        loss_fake = criterion_GAN(Disc_Y(fake_Y.detach()), torch.zeros_like(Disc_Y(fake_Y)))\n",
    "        loss_D_Y = (loss_real + loss_fake) * 0.5\n",
    "        loss_D_Y.backward()\n",
    "        optimizer_D_Y.step()\n",
    "        \n",
    "        ## Store losses\n",
    "        losses_G.append(loss_G.item())\n",
    "        losses_D_X.append(loss_D_X.item())\n",
    "        losses_D_Y.append(loss_D_Y.item())\n",
    "        losses_cycle.append(loss_cycle.item())\n",
    "        losses_identity.append((loss_id_X + loss_id_Y).item())\n",
    "        \n",
    "        if iteration % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(12,6))\n",
    "            \n",
    "            plt.plot(losses_G, label=\"Generator Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Training Losses (Epoch {epoch+1}/{epochs})\")\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(losses_D_X, label=\"Discriminator X Loss\")\n",
    "            plt.plot(losses_D_Y, label=\"Discriminator Y Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Training Losses (Epoch {epoch+1}/{epochs})\")\n",
    "            plt.show()\n",
    "            \n",
    "            show_two_images(real_X, fake_Y, nrow=2, titles=(\"Real X\", \"Fake Y\"))\n",
    "        iteration += 1\n",
    "\n",
    "        \n",
    "    # -----------------------\n",
    "    # Save models every few epochs\n",
    "    # -----------------------\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_dir = f\"models/{dataset_folder}_{gen_type}/epoch_{epoch}\"\n",
    "\n",
    "        # create directories if they don’t exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # now save your weights\n",
    "        torch.save(G_XtoY.state_dict(), os.path.join(save_dir, \"G_XtoY.pth\"))\n",
    "        torch.save(G_YtoX.state_dict(), os.path.join(save_dir, \"G_YtoX.pth\"))\n",
    "        torch.save(Disc_X.state_dict(), os.path.join(save_dir, \"Disc_X.pth\"))\n",
    "        torch.save(Disc_Y.state_dict(), os.path.join(save_dir, \"Disc_Y.pth\"))\n",
    "        print(f\"Saved models at epoch {epoch+1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "margenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
